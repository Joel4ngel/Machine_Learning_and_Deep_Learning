{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seven Steps in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Frame the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know what the customer is requiring to get out of the data. So we will identify our target or define classes or groups. Whether we are to use supervised or unsupervised machine learning.\n",
    "Whether it is a regression or a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Obtain the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to input the data from the sources the customer provides. Some of the sources forms:\n",
    "- Excel Sheets like comma separated values (CSV files)\n",
    "- SQL Databases\n",
    "\n",
    "!wget http://...data.csv   \n",
    "\n",
    "Load the data to a Dataframe using pandas:   \n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "Review the data using functions as:\n",
    "- data.info() to visualize the number of columns and rows, types, and non-null records.\n",
    "- data.describe() to review the mean, std in each column \n",
    "- data.head() and data.tail() to review how the data looks like\n",
    "- data.shape to find quickly the number of rows and columns we are dealing to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Analyze the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this step we can use functions as:\n",
    "\n",
    "- ms.matrix(data) to visualize the missing values\n",
    "- seaborn.jointplot to visualize the p value and the Pearson r coeficient (correlation) between the two columns (features)\n",
    "- seaborn.distplot to visualize the density function graph of a specific feature\n",
    "- seaborn.heatmap to visualize the correlation from cold to warm between all the features.\n",
    "- seaborn.swarm to visualize the swarm graph (points of concentration) between two features (Age: Categorical Ordinal Data, Pclass: Categorical Nominal) \n",
    "- seaborn.countplot to visualize in a bar chart the Categorical data\n",
    "- data.hist() to visualize one specific column or feature histogram\n",
    "- seaborn.boxplot to visualize the percentiles of data betwen two features\n",
    " \n",
    "Seaborn to visualize the correlation between variables, histogram of each column, swarmflight diagram\n",
    "\n",
    "https://towardsdatascience.com/data-types-in-statistics-347e152e8bee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with Missing Values\n",
    "- Review the columns with missing values using data['Cabin'].value_counts()\n",
    "- Impute the missing values where necessary using .apply(impute_age,axis=1)\n",
    "- Visualize the missing values using missingo.matrix()\n",
    "- Drop the features you are not going to use .drop('Cabin', axis=1, inplace=True)\n",
    "- Review there are not missing values .isnull().sum()\n",
    "- Drop the rows that you are not going to use .dropna(inplace=True)\n",
    "- Visualize the missing value using missingo.matrix()     \n",
    "\n",
    "\n",
    "Convert Categorical Features\n",
    "- Review the Categorical Ordinal Data using .value_counts(). That will give you the number elements of each category in the column (feature).\n",
    "- Get out the dummy values using sex = pd.get_dummmies(data['Sex'], drop_first=1)\n",
    "- Optional: make a copy of your data with old_data = data.copy()   \n",
    "\n",
    "Drop the columns not used   \n",
    "- Drop the columns you are not going to use with data.drop(['Sex,'Embarked','Name','Ticket'], axis=1, inplace=True)   \n",
    "\n",
    "Concatenate the data\n",
    "- Concatenate the data that you finally are going to use with data = pd.concat([data,sex,embarked],axis=1)\n",
    "- Optional: describe the data with data.describe(); also view the information with data.head(), or data.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Test Split   \n",
    "- Use Scikit Learn to split the dataset into Train, Test datasets   \n",
    "   from sklearn.model_selection import train_test_split  \n",
    "- Define your X dataset as your data except the column target, in this case \"Survived\", and your y or target as data['Survived'].\n",
    "- Define the size of your test dataset size.\n",
    "- Apply a random_state to shuffle both the X and y in the same order.   \n",
    "   X_train, X_test, y_train, y_test = train_test_split(data.drop('Survived',axis=1), data['Survived'], test_size=0.3, random_state=101)\n",
    "- You can verify using len(y_test) that you are using the percentage established\n",
    "\n",
    "Now we are ready to choose the model\n",
    "- Use SciKit Learn to choose the model of your preference   \n",
    "  form sklearn.linear_model import LogisticRegression\n",
    "- Create a variable where your model will reside   \n",
    "    log_model = LogisticRegression\n",
    "- Fit the data to your model   \n",
    "    log_model.fit(X_train,y_train)\n",
    "- Get the coeficients   \n",
    "    log_model.coef_\n",
    "- Get the intercept    \n",
    "    log_model.intercept_     \n",
    "    \n",
    "Now we are redy to predict using the Test Data set\n",
    "- Apply the predict function   \n",
    "    y_predict = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the Confusion Matrix functions that SciKit Learn provide   \n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "- Classification report will give you a chart summary of all the scores of accuracy , recall, and f1\n",
    "- You can also import each function separately   \n",
    "    from sklearn.metrics import accuracy_score   \n",
    "    from sklearn.metrics import recall_score   \n",
    "    from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Predict on New Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to predict on New Cases. Kaggle sometimes offers you a separate dataset for validation.\n",
    "Get the data\n",
    "!wget http:// .../validation.csv\n",
    "Load the data to a Dataframe\n",
    "prod_data = pd.read_csv('production.csv')\n",
    "Review the data using:\n",
    "data.info()\n",
    "ms.matrix()\n",
    "\n",
    "Clean the Data\n",
    "- Clean the FEATURES (columns) that you are not going to use\n",
    "  prod_data.drop('Cabin', axis = 1, inplace= True)\n",
    "- Since you cannot drop any ROW then you can fill with the values\n",
    "  prod_data.fillna(prod_data['Fare'].mean(),inplace=True)\n",
    "- Impute in the ROW of Age also is another possibility.\n",
    "- Get the dummies\n",
    "- Concatenate the data\n",
    "\n",
    "Ready! Apply your model on the Clean Production Data:\n",
    "- predict1=logmodel.predict(prod_data)\n",
    "\n",
    "Then nicely create a Dataframe with the key \"Survived\" that stores the predictions \n",
    "df1=pd.DataFrame(predict1,columns=['Survived'])\n",
    "\n",
    "Another thing to do is to have the Id of the Passengers\n",
    "- df2=pd.DataFrame(prod_data['PassengerId'],columns=['PassengerId'])\n",
    "\n",
    "Okay, now ready to make your final concatenation:\n",
    "- result = pd.concat([df2,df1],axis=1)\n",
    "\n",
    "Now put your data on a CSV File to submit it to Kaggle:\n",
    "- result.to_csv('result.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
