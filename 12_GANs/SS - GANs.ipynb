{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SS - GANs.ipynb","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":["n0xmcHwoq2RJ","WsxNikbDKadm","lgi4ItFDp8PB","cUnOm_5Sq8GT","QbYMguutw0U7","2Rpa7e44q9hG","pLaE28PNqGcB","IMJIkyRlqGch","bwlH7SZAqGc1","dIQKjiAlqGdC","bzBbQPjTqGd3","xG35AZ1aqGd4","RayCVmJgrdHz","oR-3amnaqGeO","jxlMr97iqGei","1svSbe48rkAL","MxQbphc5r2fF","FtA5kul0qGew","oFrtqs9vqGez","T_fkmW6Lr5sc","8jw68DIRqGfH"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"AMBevSZ-hYc0","colab_type":"text"},"cell_type":"markdown","source":[">> **Machine Learning Academy : Seven Step Template **\n","\n","\n","\n","\n"]},{"metadata":{"colab_type":"text","id":"kI34491C3elh"},"cell_type":"markdown","source":["<center><img src=\"https://www.dropbox.com/s/r86beo9nt8xrgh9/MLA%20Seven%20Step%20Process.png?raw=1\" height=300px width=1000px></img></center>\n","---\n","\n"]},{"metadata":{"id":"cUnOm_5Sq8GT","colab_type":"text"},"cell_type":"markdown","source":["# Step - 1 : Frame The Problem"]},{"metadata":{"id":"9vQhQGknhXjO","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"k2rVS7e_hYH_","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"Ne2-Ypwa4Hii","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"QbYMguutw0U7","colab_type":"text"},"cell_type":"markdown","source":["# Step - 2 : Obtain the Data"]},{"metadata":{"id":"2Rpa7e44q9hG","colab_type":"text"},"cell_type":"markdown","source":["## Import Libraries"]},{"metadata":{"id":"DoloRktKqGbF","colab_type":"code","colab":{}},"cell_type":"code","source":["#Installing modules we need. And doing it only once.\n","import pkgutil; \n","if not pkgutil.find_loader(\"torch\"):\n","  !pip install torch \n","  !pip install torchvision \n","  #doing this to fix an error in image\n","  !pip install Pillow==4.0.0\n","  !pip install image"],"execution_count":0,"outputs":[]},{"metadata":{"id":"R8l03wUUqGbO","colab_type":"code","colab":{}},"cell_type":"code","source":["# Importing the libraries\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","from torch.autograd import Variable"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8izv57KJfyHf","colab_type":"code","colab":{}},"cell_type":"code","source":["torch.cuda.is_available() # verifies we have GPU"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MksYS8zQigKr","colab_type":"text"},"cell_type":"markdown","source":["### Getting Data"]},{"metadata":{"id":"3fW_cUVM4Pz_","colab_type":"code","colab":{}},"cell_type":"code","source":["!wget https://www.dropbox.com/s/bd3w9exb897x9ky/GANs.zip -q\n","!unzip GANs.zip >> 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FZxB9U-fNexf","colab_type":"code","colab":{}},"cell_type":"code","source":["ls -l\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q0p4YObz4hBf","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"dIQKjiAlqGdC","colab_type":"text"},"cell_type":"markdown","source":["# Step - 3 : Analyse the Data"]},{"metadata":{"id":"nu8mxul1Hd_Q","colab_type":"code","colab":{}},"cell_type":"code","source":["cd GANs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cCRuD3VoqGb6","colab_type":"code","colab":{}},"cell_type":"code","source":["ls -l"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Gn1GrNdDqGb-","colab_type":"code","colab":{}},"cell_type":"code","source":["cd data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xwDfDthBqGdD","colab_type":"code","colab":{}},"cell_type":"code","source":["ls -l"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_QxSLTESICVc","colab_type":"code","colab":{}},"cell_type":"code","source":["cd cifar-10-batches-py/"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ozatgiUKIGE2","colab_type":"code","colab":{}},"cell_type":"code","source":["ls -l"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xG35AZ1aqGd4","colab_type":"text"},"cell_type":"markdown","source":["# Step - 4 : Feature Engineering"]},{"metadata":{"id":"RayCVmJgrdHz","colab_type":"text"},"cell_type":"markdown","source":["## Feature Engineering\n","\n","We want to fill the missing values of the age in the dataset with the average age value for each of the classes. This is called data imputation."]},{"metadata":{"id":"cyRPnMhBITFn","colab_type":"code","colab":{}},"cell_type":"code","source":["# Setting some hyperparameters\n","batchSize = 64 # We set the size of the batch.\n","imageSize = 64 # We set the size of the generated images (64x64)."],"execution_count":0,"outputs":[]},{"metadata":{"id":"1VHWSiq4CLNW","colab_type":"code","colab":{}},"cell_type":"code","source":["# Creating the transformations\n","transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), \n","                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) \n","# We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"by-vtJ-MCP5X","colab_type":"code","colab":{}},"cell_type":"code","source":["# Loading the dataset\n","dataset = dset.CIFAR10(root = './data', download = True, transform = transform) \n","# We download the training set in the ./data folder and we apply the previous transformations on each image.\n","\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) \n","# We use dataLoader to get the images of the training set batch by batch.\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jxlMr97iqGei","colab_type":"text"},"cell_type":"markdown","source":["# Step - 5 : Model Creation"]},{"metadata":{"id":"gSfgFEucgZ_Y","colab_type":"text"},"cell_type":"markdown","source":["<center><img src=\"https://www.dropbox.com/s/i37mgynkrf1d3vb/supervised_flow_chart.png?raw=1\" height=300px width=1000px></img></center>"]},{"metadata":{"id":"s7u71_Yj7iIx","colab_type":"text"},"cell_type":"markdown","source":["## Train Test Split"]},{"metadata":{"id":"_zyebrQ3moR5","colab_type":"code","colab":{}},"cell_type":"code","source":["# Done Above"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1svSbe48rkAL","colab_type":"text"},"cell_type":"markdown","source":["## Building a  model"]},{"metadata":{"id":"KKzr-_boqGes","colab_type":"code","colab":{}},"cell_type":"code","source":["# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X_ZQilgtC1xR","colab_type":"code","colab":{}},"cell_type":"code","source":["# Defining the generator\n","\n","class G(nn.Module):\n","\n","    def __init__(self):\n","        super(G, self).__init__()\n","        self.main = nn.Sequential(\n","            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, input):\n","        output = self.main(input)\n","        return output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oNv3n66sI15g","colab_type":"code","colab":{}},"cell_type":"code","source":[" # Creating the generator\n","netG = G()\n","netG.apply(weights_init)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rvr-bsh6I5XS","colab_type":"code","colab":{}},"cell_type":"code","source":["# Defining the discriminator\n","\n","class D(nn.Module):\n","\n","    def __init__(self):\n","        super(D, self).__init__()\n","        self.main = nn.Sequential(\n","            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n","            nn.LeakyReLU(0.2, inplace = True),\n","            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace = True),\n","            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace = True),\n","            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace = True),\n","            nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        output = self.main(input)\n","        return output.view(-1)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GSGrdihsI81w","colab_type":"code","colab":{}},"cell_type":"code","source":["# Creating the discriminator\n","netD = D()\n","netD.apply(weights_init)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IIJ8rqCrJAMx","colab_type":"code","colab":{}},"cell_type":"code","source":["# Training the DCGANs\n","\n","criterion = nn.BCELoss()\n","optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n","optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H7yxEDnPvBrx","colab_type":"code","colab":{}},"cell_type":"code","source":["cd /content/GANs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HmmMjqwJJHaP","colab_type":"code","colab":{}},"cell_type":"code","source":["for epoch in range(25):\n","\n","    for i, data in enumerate(dataloader, 0):\n","        \n","        # 1st Step: Updating the weights of the neural network of the discriminator\n","\n","        netD.zero_grad()\n","        \n","        # Training the discriminator with a real image of the dataset\n","        real, _ = data\n","        input = Variable(real)\n","        target = Variable(torch.ones(input.size()[0]))\n","        output = netD(input)\n","        errD_real = criterion(output, target)\n","        \n","        # Training the discriminator with a fake image generated by the generator\n","        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n","        fake = netG(noise)\n","        target = Variable(torch.zeros(input.size()[0]))\n","        output = netD(fake.detach())\n","        errD_fake = criterion(output, target)\n","        \n","        # Backpropagating the total error\n","        errD = errD_real + errD_fake\n","        errD.backward()\n","        optimizerD.step()\n","\n","        # 2nd Step: Updating the weights of the neural network of the generator\n","\n","        netG.zero_grad()\n","        target = Variable(torch.ones(input.size()[0]))\n","        output = netD(fake)\n","        errG = criterion(output, target)\n","        errG.backward()\n","        optimizerG.step()\n","        \n","        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n","\n","        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.item(), errG.item()))\n","        if i % 100 == 0:\n","            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True)\n","            fake = netG(noise)\n","            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vR440WOk8Gpm","colab_type":"text"},"cell_type":"markdown","source":["## Predict using Model"]},{"metadata":{"id":"S8bdMGNbqGet","colab_type":"code","colab":{}},"cell_type":"code","source":["# Testing the SAE\n","test_loss = 0\n","s = 0.\n","for id_user in range(nb_users):\n","    input = Variable(training_torch[id_user]).unsqueeze(0)\n","    target = Variable(test_torch[id_user])\n","    if torch.sum(target.data > 0) > 0:\n","        output = sae(input)\n","        output = sae(input).view(output.shape[1]) # did to this to get same shape as input\n","        target.require_grad = False\n","        output[target == 0] = 0\n","        loss = criterion(output, target)\n","        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n","        test_loss += np.sqrt(loss.item()*mean_corrector)\n","        s += 1.\n","print('test loss: '+str(test_loss/s))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_YKIZMdEqGev","colab_type":"text"},"cell_type":"markdown","source":["Let's move on to evaluate our model."]},{"metadata":{"id":"MxQbphc5r2fF","colab_type":"text"},"cell_type":"markdown","source":["# Step - 6 : Evaluation"]},{"metadata":{"id":"ctxrXoy3DLaB","colab_type":"code","colab":{}},"cell_type":"code","source":["print('test loss: '+str(test_loss/s))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c5i2Cgv8DTE5","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}